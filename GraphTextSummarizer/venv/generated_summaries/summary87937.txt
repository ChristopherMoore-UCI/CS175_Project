Certainly, the far-left feminist movement has sought to diminish the role of men, but a majority of women want able, competent men of their equal. Strong men make stronger women (and vice versa) and stronger families, and women want that. And for the first time, American women now gain more advanced college degrees, as well as bachelor's degrees, than American men. In all these shows, men have become the butt of the jokes. Although men remain at the top of the heap in terms of compensation and job status, particularly in fields like science, business, and politics, things are changing.